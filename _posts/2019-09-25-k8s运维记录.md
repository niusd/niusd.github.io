---
layout: mypost
title: k8s运维问题记录
categories: [k8s]
---

[toc]


## 1.k8s集群节点taints（污点）添加容忍的方法
查看集群总共有多少pod
```sh
kubectl get pod --all-namespaces |wc -l
```
查看哪个node节点有污点
```sh
kubectl get node -oyaml|grep taint -C 10
```
编辑有污点的节点的配置文件，删掉污点的配置项即可
```sh
kubectl edit node k8s-ctl03
```

## 2.calico未启动问题解决
calico正常状态为UP状态，查看命令为
```sh
calicoctl node status
```
如果有节点的calico状态不是UP的话，需要起开一下，启动命令（替换一下nodename和IP）：
```sh
docker run --net=host --privileged --name=calico-node -e NODENAME=k8s-ctl03 -e IP=172.16.11.153 -e IP6= -e AS=64512 -e NO_DEFAULT_POOLS=False -e CALICO_STARTUP_LOGLEVEL=INFO -e CLUSTER_TYPE=k8s,bgp -e CALICO_LIBNETWORK_ENABLED=False -e ETCD_ENDPOINTS=https://172.16.11.151:4001,https://172.16.11.152:4001,https://172.16.11.153:4001 -e ETCD_CA_CERT_FILE=/var/lib/etcd/ca.pem -e ETCD_CERT_FILE=/var/lib/etcd/etcd-client.crt -e ETCD_KEY_FILE=/var/lib/etcd/etcd-client.key -v /var/lib/etcd/:/var/lib/etcd/:ro -e FELIX_PROMETHEUSMETRICSENABLED=true -e FELIX_PROMETHEUSMETRICSPORT=9091 -p 0.0.0.0:9091:9091 -v /var/log/calico:/var/log/calico -v /var/lib/calico:/var/lib/calico -v /run/docker/plugins:/run/docker/plugins -v /lib/modules:/lib/modules -v /var/run/calico:/var/run/calico apt:5000/mirantis/projectcalico/calico/node:v3.1.3 &
```
上面命令后面跟着“&”，表示后台启动
启动成功会显示Calico node started successfully

## 3.storageclass手动添加
ceph-default storageclass配置文件：storage_class_ceph.yml
```sh
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    displayName: ceph-default
    storageclass.beta.kubernetes.io/is-default-class: "false"
  creationTimestamp: 2019-03-22T00:58:13Z
  name: ceph-default
  resourceVersion: "10588354"
  selfLink: /apis/storage.k8s.io/v1/storageclasses/ceph-default
  uid: 919f83f3-4c3d-11e9-a1e4-50af732e0e17
parameters:
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  imageFormat: "1"
  monitors: 172.16.12.151:6789,172.16.12.152:6789,172.16.12.153:6789
  pool: rbd
  userId: admin
  userSecretName: ceph-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
```
使用方法：

* `kubectl create -f storage_class_ceph.yml`

## 4.k8s中pod的pvc创建不出来，一直处于pending状态
- 检查ceph集群状态  
ceph status
- 检查后ceph状态为err
- 查看detail详情 ceph health detail
```
HEALTH_ERR 1 full osd(s); 6 pool(s) full; clock skew detected on mon.k8s-ctl02, mon.k8s-ctl03
OSD_FULL 1 full osd(s)
    osd.13 is full
POOL_FULL 6 pool(s) full
    pool 'rbd' is full (no space)
    pool 'cephfs_data' is full (no space)
    pool 'cephfs_metadata' is full (no space)
    pool '.rgw.root' is full (no space)
    pool 'default.rgw.meta' is full (no space)
    pool 'default.rgw.log' is full (no space)
MON_CLOCK_SKEW clock skew detected on mon.k8s-ctl02, mon.k8s-ctl03
    mon.k8s-ctl02 addr 172.16.12.152:6789/0 clock skew 0.405838s > max 0.05s (latency 0.000865258s)
    mon.k8s-ctl03 addr 172.16.12.153:6789/0 clock skew 0.405109s > max 0.05s (latency 0.000536627s)
```
看到有一个osd满了

- 使用命令 ceph osd df查看每个osd使用状态，看到osd.13使用超过95%，**根据ceph官方文档描述，当一个osd full比例达到95%时，集群将不接受任何ceph client的读写数据请求**。
- 暂时解决方法：根本解决是增加osd容量，如果只有一两个osd满了的话 可以通过调节osd的weight的方法，是osd上的数据分到其他osd上
```
ceph osd crush reweight osd.13 0.3
```
设置osd.13的权重为0.3  
详细查看https://blog.csdn.net/JackLiu16/article/details/81487990

## 5.给node设置taint
```
kubectl taint node [node] key=value[effect]   
     其中[effect] 可取值: [ NoSchedule |    PreferNoSchedule | NoExecute ]
      NoSchedule: 一定不能被调度
      PreferNoSchedule: 尽量不要调度
      NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod
```

删除taint
```
kubectl taint node node1 key1:NoSchedule-  # 这里的key可以不用指定value
kubectl taint node node1 key1:NoExecute-
# kubectl taint node node1 key1-  删除指定key所有的effect
kubectl taint node node1 key2:NoSchedule-
```